{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "78hwtX74aaeN",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7X7Njio92KFR"
   },
   "source": [
    "# FCN\n",
    "\n",
    "Avui farem feina amb xarxes que no tenen cap tipus de capa _fully connected_ per tant serà una xarxa _Fully Convolutional Network_ (FCN). Quan parlam d'una xarxa FCN, ens referim a xarxes tipus VGG. Ens anirà molt bé fer aquesta pràctica per poder passar a xarxes que fan segmentació ja que la meitat d'aquestes és una FCN.\n",
    "\n",
    "Emprarem un dataset propi per fer aquesta pràctica. Això implica fer una mica més de feina per preparar les dades. En concret emprarem una versió del conjunt de dades : AIXI_SHAPE propi d'en Miquel Miró. [Enllaç](https://uibes-my.sharepoint.com/:u:/g/personal/gma040_id_uib_eu/EcsNAK5mkXRBqayDo1JYeooBWCf1lpRA-YJHT_kDF4J_nA?e=apkCql)\n",
    "\n",
    "La feina d'avui és \"lliure\" (considerau-ho una mini-pràctica), el conjunt de dades que teniu a la vostra disposició permet fer com a mínim 4 feines:\n",
    "\n",
    "1. **Regressió**: Contar quants d'objectes hi ha\n",
    "2. **Regressió de classe**: Contar quants d'objectes de cada classe hi ha en una imatge.\n",
    "3. **Detecció**: Mostrar on hi ha cada un dels objectes. Es podrien emprar xarxes ja fetes per aquesta tasca (tant les que teniu disponibles a pytorch com altres que trobeu)\n",
    "4. **Segmentació**: Encara no en sabem, però ho resoldrem la setmana que vé.\n",
    "\n",
    "Avui heu de fer una de les dues primeres. Tant podeu triar fer-ho amb les imatges amb textura, com amb les imatges binaries que serveixen com a _ground truth_ (gt).\n",
    "\n",
    "Les imatges del gt són imatges binàries (0,1) de 3 canals on a cada canal hi ha un tipus d'objecte . Per poder contar el nombre d'objectes possiblement haureu de emprar les funcions `cv2.add` per unir tots els canals en una sola imatge i la funció `cv2.findContours` per contar el nombre d'objectes en una imatge. A més podeu demanar-me ajuda a mi o al vostre amic ChatGPT.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### [Inciso] Si emprau Colab:\n",
    "\n",
    "Aquest codi us serveix per connectar colab amb google drive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mGq0Ys7EGSTs",
    "outputId": "0639e06a-4fdb-4191-9aff-2715abc80ecf"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FkUHh4I4HyEQ",
    "outputId": "c8670a2f-1365-4943-ca58-4f6cd68bc40c"
   },
   "outputs": [],
   "source": [
    "%ls\n",
    "%cd #TODO al vostre sistema de fitxers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5IUXSYLw07AV"
   },
   "source": [
    "## Preparació de les Dades\n",
    "Per preparar el conjunt de dades necessitarem fer algunes pases:\n",
    "\n",
    "1. Crear una llista amb les imatges \n",
    "2. Crear una classe que ens permeti obtenir una tupla (imatge, etiqueta)\n",
    "3. Emprar els objectes DataLoader com hem fet sempre, aquí no trobareu cap canvi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crear una llista amb les imatges \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "CvBbWj9BZONG",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path_train = \"aixi_shape_256_texture/train/\"\n",
    "\n",
    "files = os.listdir(path_train)\n",
    "img_files = list([f\"{path_train}{p}\" for p in files if p.endswith('.png')])\n",
    "label_files = list([f\"{path_train}gt/{p}\" for p in files if p.endswith('.png')])\n",
    "\n",
    "import os\n",
    "\n",
    "path_test = \"aixi_shape_256_texture/val/\"\n",
    "\n",
    "files = os.listdir(path_test)\n",
    "test_img_files = list([f\"{path_test}{p}\" for p in files if p.endswith('.png')])\n",
    "test_label_files = list([f\"{path_test}gt/{p}\" for p in files if p.endswith('.png')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2B2JRO5T1ZiS"
   },
   "source": [
    "#### Crear una classe que ens permeti obtenir una tupla (imatge, etiqueta)\n",
    "\n",
    "Aquesta classe hereta de la superclasse _Dataset_ i com a mínim ha de tenir els mètodes:\n",
    "\n",
    "1. `__len__(self)`: retorna la longitud del dataset\n",
    "2. `__getitem__(self, index)`: retorna l'element que es troba a la posició marcada pel valor d'index. Quan parlam d'un element parlam de la imatge i de la seva etiqueta.\n",
    "\n",
    "El constructor i els atributs de la classe els he decidit jo:\n",
    "\n",
    "- Llista amb els _paths_ a les imatges\n",
    "- Llista amb els _paths_ a les imatges de gt que ens serviràn per calcular l'etiqueta de la imatge\n",
    "- Un objecte transform\n",
    "\n",
    "A la classe podeu afegir tants mètodes públics i privats com necessiteu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "JiVfQJ0ZbzD0",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 224, 224])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Constructor del dataset.\n",
    "class AIXI_Shape(Dataset):\n",
    "    def __init__(self, images, labels, transform):\n",
    "        super().__init__()\n",
    "        self.paths = images\n",
    "        self.labels = labels\n",
    "        self.len = len(self.paths)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self): \n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = cv2.imread(self.paths[index])\n",
    "        image = cv2.resize(image, (224, 224), interpolation = cv2.INTER_AREA)\n",
    "        \n",
    "        # TODO: verificar? No fa res?\n",
    "        gt_image = cv2.imread(self.labels[index])\n",
    "        b,g,r = cv2.split(gt_image)\n",
    "        gt_image = cv2.add(b,g,r)\n",
    "        \n",
    "        contours, _= cv2.findContours(gt_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        num_shapes = len(contours)\n",
    "\n",
    "        # It is not necessary to use torch.tensor since the batch automagically assigns that\n",
    "        label = num_shapes\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return (image, label)\n",
    "\n",
    "# image normalization\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# creació dels conjunts d'entrenament i test\n",
    "train_ds = AIXI_Shape(img_files, label_files, transform)\n",
    "train_dl = DataLoader(train_ds, batch_size=64)\n",
    "train_dl.dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 224, 224])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds = AIXI_Shape(test_img_files, test_label_files, transform)\n",
    "test_dl = DataLoader(test_ds, batch_size=64)\n",
    "test_dl.dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kUKSc7YL3kNK"
   },
   "source": [
    "## Xarxa\n",
    "Com sempre, vosaltres us encarregau de dissenyar la xarxa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Jo2R5YE8ifMP"
   },
   "outputs": [],
   "source": [
    "class MyNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MyNet, self).__init__()\n",
    "        # https://blog.paperspace.com/vgg-from-scratch-pytorch/\n",
    "        # https://www.kaggle.com/code/datastrophy/vgg16-pytorch-implementation\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1, stride=1), # 224x224x3 -> 224x224x64\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1, stride=1), # 224x224x64 -> 224x224x64\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), # 224x224x64 -> 112x112x64\n",
    "            \n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1, stride=1), # 112x112x64 -> 112x112x128\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1, stride=1), # 112x112x128 -> 112x112x128\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), # 112x112x128 -> 56x56x128\n",
    "            \n",
    "            #nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1, stride=1), # 56x56x128 -> 56x56x256\n",
    "            #nn.ReLU(inplace=True),\n",
    "            #nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1, stride=1), # 56x56x256 -> 56x56x256\n",
    "            #nn.ReLU(inplace=True),\n",
    "            #nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1, stride=1), # 56x56x256 -> 56x56x256\n",
    "            #nn.ReLU(inplace=True),\n",
    "            #nn.MaxPool2d(kernel_size=2, stride=2), # 56x56x256 -> 28x28x256\n",
    "            \n",
    "            #nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1, stride=1), # 28x28x256 -> 28x28x512\n",
    "            #nn.ReLU(inplace=True),\n",
    "            #nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1, stride=1), # 28x28x512 -> 28x28x512\n",
    "            #nn.ReLU(inplace=True),\n",
    "            #nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1, stride=1), # 28x28x512 -> 28x28x512\n",
    "            #nn.ReLU(inplace=True),\n",
    "            #nn.MaxPool2d(kernel_size=2, stride=2), # 28x28x512 -> 14x14x512\n",
    "            \n",
    "            #nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1, stride=1), # 14x14x512 -> 14x14x512\n",
    "            #nn.ReLU(inplace=True),\n",
    "            #nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1, stride=1), # 14x14x512 -> 14x14x512\n",
    "            #nn.ReLU(inplace=True),\n",
    "            #nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1, stride=1), # 14x14x512 -> 14x14x512\n",
    "            #nn.ReLU(inplace=True),\n",
    "            #nn.MaxPool2d(kernel_size=2, stride=2), # 14x14x512 -> 7x7x512           \n",
    "            \n",
    "            nn.Conv2d(in_channels=128, out_channels=4, kernel_size=3, padding=1, stride=1),  # 56x56x128 -> 56x56x128\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=56, stride=56)  # 7x7x1 -> 1x1x1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NQVfN-vD3uPF"
   },
   "source": [
    "# Entrenament\n",
    "\n",
    "El blucle d'entrenament és el de sempre. Només heu de pensar quina funció de pèrdua heu d'emprar per el vostre/nostre problema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "3WqMHALIoN1c"
   },
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch, log_interval=100, verbose=True):\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    loss_v = 0\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    \n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "      \n",
    "        # Cross entropy needs shapes to match?\n",
    "        print(output.shape)\n",
    "        print(target.shape)\n",
    "        \n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "      \n",
    "        loss_v += loss.item()\n",
    "\n",
    "    loss_v /= len(train_loader.dataset)\n",
    "    print('\\nTrain set: Average loss: {:.4f}\\n'.format(loss_v))\n",
    " \n",
    "    return loss_v\n",
    "\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            \n",
    "            test_loss += F.cross_entropy(output, target_resized, reduction=\"sum\")\n",
    "          \n",
    "   \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    \n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-3yxe2hX4eGc"
   },
   "source": [
    "## Entrenament"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "8jGCuUhwoR7A"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cpu\n",
      "torch.Size([64, 4, 1, 1])\n",
      "torch.Size([64])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "only batches of spatial targets supported (3D tensors) but got targets of dimension: 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-1c38a8cd41e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Bucle d'entrenament\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mtrain_l\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mtest_l\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-0354d125e7e0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, optimizer, epoch, log_interval, verbose)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3024\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3025\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: only batches of spatial targets supported (3D tensors) but got targets of dimension: 1"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(33)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device {device}\")\n",
    "\n",
    "epochs = 15\n",
    "lr =0.00001\n",
    "\n",
    "model = MyNet().to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Guardam el valor de pèrdua mig de cada iteració (època)\n",
    "train_l = np.zeros((epochs))\n",
    "test_l = np.zeros((epochs))\n",
    "\n",
    "# Bucle d'entrenament\n",
    "for epoch in range(0, epochs):\n",
    "    train_l[epoch] = train(model, device, train_dl, optimizer, epoch)\n",
    "    test_l[epoch]  = test(model, device, test_dl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YaBP84cn4tPK"
   },
   "source": [
    "## Validació\n",
    "\n",
    "Heu de fer vosaltres la validació depenent del problema que voldreu resoldre"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
