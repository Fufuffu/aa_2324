{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.optim import Adam, SGD\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import random\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import pylab as pl\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentació d'objectes emprant Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # Una manera diferent d'activar la GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feina a fer**\n",
    "\n",
    "Un cop teniu els conjunts de dades creats heu de comprovar que les imatges que es corresponen amb les etiquetes tenen la informació correcta, feis una visualització."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training images 500\n",
      "total test images 2000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Dades entrenament\n",
    "path_train = \"../09_FCN/aixi_shape_256_texture/train/\"\n",
    "files = os.listdir(path_train)\n",
    "img_files = list([f\"{path_train}{p}\" for p in files if p.endswith('.png')])[:500]\n",
    "label_files = list([f\"{path_train}gt/{p}\" for p in files if p.endswith('.png')])[:500]\n",
    "print(\"total training images\", len(img_files))\n",
    "\n",
    "# Dades validacio\n",
    "\n",
    "path_val = \"../09_FCN/aixi_shape_256_texture/val/\"\n",
    "files = os.listdir(path_val)\n",
    "img_files_val = list([f\"{path_val}{p}\" for p in files if p.endswith('.png')])\n",
    "label_files_val = list([f\"{path_val}gt/{p}\" for p in files if p.endswith('.png')])\n",
    "print(\"total test images\", len(img_files_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_batch_size = 64\n",
    "test_batch_size = 100\n",
    "\n",
    "# Definim una seqüència (composició) de transformacions \n",
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "    ## TODO: Put if necessary\n",
    "    ])\n",
    "\n",
    "# Constructor del dataset.\n",
    "class Formes(Dataset):\n",
    "    def __init__(self, images, labels, transform):\n",
    "        super().__init__()\n",
    "        self.paths = images\n",
    "        self.labels = labels\n",
    "        self.len = len(self.paths)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self): \n",
    "        return self.len\n",
    "    \n",
    "                \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        path = self.paths[index]\n",
    "        label = self.labels[index]\n",
    "        image = cv2.imread(path) #, cv2.IMREAD_GRAYSCALE)  # Depén de vosaltres\n",
    "        \n",
    "        image = self.transform(image)\n",
    "        label_img = cv2.imread(label)\n",
    "\n",
    "        merged_image = cv2.add(label_img[:,:,0], cv2.add(label_img[:,:,1], label_img[:,:,2]))\n",
    "        \n",
    "        label_img = self.transform(merged_image)\n",
    "        return image, label_img\n",
    "    \n",
    "train_data = Formes(img_files, label_files, transform)\n",
    "val_data = Formes(img_files_val, label_files_val, transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, train_batch_size)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, test_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "iterador =  iter(val_loader) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saber l'estructura del batch us ajudarà: \n",
      "Feature batch shape: torch.Size([100, 3, 256, 256])\n",
      "Labels batch shape: torch.Size([100, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "features, labels = next(iterador)\n",
    "\n",
    "print(\"Saber l'estructura del batch us ajudarà: \")\n",
    "print(f\"Feature batch shape: {features.size()}\")\n",
    "print(f\"Labels batch shape: {labels.size()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: mostrar una imatge del batch i devora mostrar l'imatge que fa d'etiqueta\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definició de la xarxa\n",
    "\n",
    "Podem observar com es pot emprar l'orientació a objectes de **Python** per crear una xarxa de manera ordenada, és interessant analitzar aquest codi amb detall ja que en podem aprendre molt:\n",
    "\n",
    "Aquí tenim 2 capes noves que ens ajudaràn a construïr la nova arquitectura:\n",
    "\n",
    "- [ConvTranspose2d](https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html): Aplica un operador de convolució transposat 2D sobre una imatge d'entrada composta per diversos plans d'entrada.    [Exemples gràfics](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md).\n",
    "- [BatchNorm2d](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html): És una tècnica utilitzada en l'entrenament de xarxes neuronals artificials per a estabilizar i accelerar el procés de convergència durant l'entrenament. Bàsicament, durant l'entrenament d'una red neuronal, els valors d'entrada de cada capa poden canviar a mesura que els paràmetres de les capes anteriors s'actualitzen. Això pot fer que l'entrenament sigui més lento o inestable. La normalització per lots resuelve això normalitzant les activacions de cada capa abans de passar a la següent capa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class UNet(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels=3, out_channels=1, init_features=32):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        features = init_features\n",
    "        self.encoder1 = UNet._block(in_channels, features, name=\"enc1\")\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.encoder2 = UNet._block(features, features * 2, name=\"enc2\")\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.encoder3 = UNet._block(features * 2, features * 4, name=\"enc3\")\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.encoder4 = UNet._block(features * 4, features * 8, name=\"enc4\")\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.bottleneck = UNet._block(features * 8, features * 16, name=\"bottleneck\")\n",
    "\n",
    "        self.upconv4 = nn.ConvTranspose2d(\n",
    "            features * 16, features * 8, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.decoder4 = UNet._block((features * 8) * 2, features * 8, name=\"dec4\")\n",
    "        self.upconv3 = nn.ConvTranspose2d(\n",
    "            features * 8, features * 4, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.decoder3 = UNet._block((features * 4) * 2, features * 4, name=\"dec3\")\n",
    "        self.upconv2 = nn.ConvTranspose2d(\n",
    "            features * 4, features * 2, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.decoder2 = UNet._block((features * 2) * 2, features * 2, name=\"dec2\")\n",
    "        self.upconv1 = nn.ConvTranspose2d(\n",
    "            features * 2, features, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.decoder1 = UNet._block(features * 2, features, name=\"dec1\")\n",
    "\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=features, out_channels=out_channels, kernel_size=1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc2 = self.encoder2(self.pool1(enc1))\n",
    "        enc3 = self.encoder3(self.pool2(enc2))\n",
    "        enc4 = self.encoder4(self.pool3(enc3))\n",
    "\n",
    "        bottleneck = self.bottleneck(self.pool4(enc4))\n",
    "\n",
    "        dec4 = self.upconv4(bottleneck)\n",
    "        dec4 = torch.cat((dec4, enc4), dim=1)\n",
    "        dec4 = self.decoder4(dec4)\n",
    "        dec3 = self.upconv3(dec4)\n",
    "        dec3 = torch.cat((dec3, enc3), dim=1)\n",
    "        dec3 = self.decoder3(dec3)\n",
    "        dec2 = self.upconv2(dec3)\n",
    "        dec2 = torch.cat((dec2, enc2), dim=1)\n",
    "        dec2 = self.decoder2(dec2)\n",
    "        dec1 = self.upconv1(dec2)\n",
    "        dec1 = torch.cat((dec1, enc1), dim=1)\n",
    "        dec1 = self.decoder1(dec1)\n",
    "        return torch.sigmoid(self.conv(dec1))\n",
    "\n",
    "    @staticmethod\n",
    "    def _block(in_channels, features, name):\n",
    "        return nn.Sequential(\n",
    "            OrderedDict(\n",
    "                [\n",
    "                    (\n",
    "                        name + \"conv1\",\n",
    "                        nn.Conv2d(\n",
    "                            in_channels=in_channels,\n",
    "                            out_channels=features,\n",
    "                            kernel_size=3,\n",
    "                            padding=1,\n",
    "                            bias=False,\n",
    "                        ),\n",
    "                    ),\n",
    "                    (name + \"norm1\", nn.BatchNorm2d(num_features=features)),\n",
    "                    (name + \"relu1\", nn.ReLU(inplace=True)),\n",
    "                    (\n",
    "                        name + \"conv2\",\n",
    "                        nn.Conv2d(\n",
    "                            in_channels=features,\n",
    "                            out_channels=features,\n",
    "                            kernel_size=3,\n",
    "                            padding=1,\n",
    "                            bias=False,\n",
    "                        ),\n",
    "                    ),\n",
    "                    (name + \"norm2\", nn.BatchNorm2d(num_features=features)),\n",
    "                    (name + \"relu2\", nn.ReLU(inplace=True)),\n",
    "                ]\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenament\n",
    "\n",
    "Per fer tasques de segmentació, una de les funcions de pèrdua que podem emprar és el _Diceloss_ (intersecció vs unió):  El coeficient de _Dice_ s'utilitza habitualment en tasques de segmentació d'imatges com a mesura de la superposició entre les màscares de segmentació entre la predicció i el _ground truth_. El  _Diceloss_, és el complementari del coeficient de _Dice_, es pot utilitzar com a funció de pèrdua per entrenar models per a tasques de segmentació.\n",
    "\n",
    "Dice Coefficient=$ = 2 \\times \\frac{|X \\cap Y|}{|X| + |Y|}$\n",
    "\n",
    "\n",
    "\n",
    "On:\n",
    "\n",
    "- $X$ és la màscara de segmentació prevista.\n",
    "- $Y$ és la màscara de segmentació de la veritat del sòl.\n",
    "- $∣⋅∣$ denota la cardinalitat o el nombre d'elements d'un conjunt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = 0.0\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        assert y_pred.size() == y_true.size()\n",
    "        y_pred = y_pred[:, 0].contiguous().view(-1)\n",
    "        y_true = y_true[:, 0].contiguous().view(-1)\n",
    "        intersection = (y_pred * y_true).sum()\n",
    "        dsc = (2. * intersection + self.smooth) / (\n",
    "            y_pred.sum() + y_true.sum() + self.smooth\n",
    "        )\n",
    "        return 1. - dsc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El bucle d'entrenament és diferent al que estau acostumats a veure en l'assignatura, s'assembla molt més als propis tutorials de _Pytorch_.\n",
    "\n",
    "A més s'aprofita per introduir la visualització de resultats de forma dinàmica usant la llibreria [tqdm](https://github.com/tqdm/tqdm) i la llibreria _matplotlib_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "use_cuda = True\n",
    "torch.manual_seed(33)\n",
    "\n",
    "if use_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:2 Training Loss:0.9995562508702278 Validation Loss:0.9997777074575425:   5%|▏  | 2/40 [15:54<5:10:27, 490.20s/it]"
     ]
    }
   ],
   "source": [
    "epochs = 40\n",
    "\n",
    "model = UNet().to(device)\n",
    "\n",
    "optim = Adam(model.parameters(), lr=1e-3)\n",
    "criterion = DiceLoss() \n",
    "\n",
    "t_loss = np.zeros((epochs))\n",
    "v_loss = np.zeros((epochs))\n",
    "\n",
    "pbar = tqdm(range(1, epochs+1)) # tdqm permet tenir text dinàmic\n",
    "\n",
    "for epoch in pbar:\n",
    "    \n",
    "    train_loss = 0 \n",
    "    val_loss = 0  \n",
    "    \n",
    "    model.train()                                                  \n",
    "    for batch_num, (input_img, target) in enumerate(train_loader, 1):   \n",
    "        input_img= input_img.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        output = model(input_img)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()                                            \n",
    "        optim.step()                                               \n",
    "        optim.zero_grad()     \n",
    "        \n",
    "        train_loss += loss.item()    \n",
    "                                                       \n",
    "    model.eval()   \n",
    "    with torch.no_grad():                                          \n",
    "        for input_img, target in val_loader: \n",
    "            input_img = input_img.to(device)\n",
    "            target = target.to(device)\n",
    "            \n",
    "            output = model(input_img)                                   \n",
    "            loss = criterion(output, target)   \n",
    "            val_loss += loss.item()  \n",
    "    \n",
    "    # RESULTATS\n",
    "    train_loss /= len(train_loader)\n",
    "    t_loss[epoch-1] = train_loss\n",
    "    \n",
    "    val_loss /= len(val_loader)   \n",
    "    v_loss[epoch-1] = val_loss\n",
    "    \n",
    "    # VISUALITZACIO DINAMICA\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    pl.plot(t_loss[:epoch], label=\"train\")\n",
    "    pl.plot(v_loss[:epoch], label=\"validation\")\n",
    "    pl.legend()\n",
    "    pl.xlim(0, epochs)\n",
    "    pl.xticks(range(0,epochs,1),range(1,epochs+1,1))\n",
    "    \n",
    "    display.clear_output(wait=True)\n",
    "    display.display(pl.gcf())\n",
    "    plt.close()\n",
    "\n",
    "    pbar.set_description(f\"Epoch:{epoch} Training Loss:{train_loss} Validation Loss:{val_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardam el model, d'aquesta manera no es necessari fer l'entrenament a classe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"unet.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaluació"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregam el model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmodel =  UNet().to(device)\n",
    "mmodel.load_state_dict(torch.load(\"unet.pt\"))\n",
    "mmodel.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feina a fer**\n",
    "\n",
    "Visualitzar exemples de segmentació."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "[agraïments Unet](https://github.com/mateuszbuda/brain-segmentation-pytorch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
